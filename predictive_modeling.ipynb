{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import h3\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import papermill as pm\n",
    "import concurrent.futures\n",
    "import random\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, train_ratio=0.75, validation_ratio=0.15, random_state=42):\n",
    "    features = ['demand_h-2', 'demand_h-24', 'hour_sin', 'hour_cos', 'weekend', 'season_sin', 'season_cos', 'public_holiday', 'temperature', 'precip']\n",
    "    features_to_scale = ['demand_h-2', 'demand_h-24', 'temperature', 'precip']\n",
    "    target = 'demand'\n",
    "    \n",
    "    # Copy the input DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Select features and target\n",
    "    X = df_copy[features]\n",
    "    y = df_copy[target]\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "\n",
    "    test_ratio = (1-train_ratio)-validation_ratio\n",
    "    X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=(1 - train_ratio), random_state=random_state)\n",
    "    X_val_unscaled, X_test_unscaled, y_val, y_test = train_test_split(X_test_unscaled, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=random_state)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_unscaled[features_to_scale])\n",
    "\n",
    "    X_train = X_train_unscaled.copy()\n",
    "    X_val = X_val_unscaled.copy()\n",
    "    X_test = X_test_unscaled.copy()\n",
    "\n",
    "    X_train[features_to_scale] = scaler.transform(X_train_unscaled[features_to_scale])\n",
    "    X_val[features_to_scale] = scaler.transform(X_val_unscaled[features_to_scale])\n",
    "    X_test[features_to_scale] = scaler.transform(X_test_unscaled[features_to_scale])\n",
    "\n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Datasets from Feature Engineering\n",
    "# This takes around ~ 25-35 Minutes and will fill you RAM and CPU nearly completely. \n",
    "\n",
    "# Constants\n",
    "TIME_RESOLUTIONS = ['1H', '2H', '6H', '24H']\n",
    "SPATIAL_RESOLUTIONS = [6, 7, 8]\n",
    "DATASET_SUFFIX = ['_h3', '_census']\n",
    "PROCESSING_NOTEBOOK_FILE = './predicitve_feature_engineering.ipynb'\n",
    "FILE_BASE_NAME='./data/predictive/dataset'\n",
    "# Max 4 on 32 GB Ram (Adrians Machine)\n",
    "MAX_WORKER_THREADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ^ supress notebook outputs as to not get spammed by 12 Data preparation notebooks. Output can be found under /data/notebook_outs\n",
    "\n",
    "output_filenames = []\n",
    "\n",
    "# Function to execute a notebook with given parameters\n",
    "def execute_notebook(notebook, params):\n",
    "    output_notebook = f\"./data/notebook_outs/output_{random.randint(1, 100)}\"\n",
    "    pm.execute_notebook(notebook, output_notebook, parameters=params)\n",
    "\n",
    "# Generate notebooks and parameters\n",
    "notebooks_and_params = []\n",
    "for time_res in TIME_RESOLUTIONS:\n",
    "    for spatial_res in SPATIAL_RESOLUTIONS:\n",
    "        output_filename_base = f'{FILE_BASE_NAME}-spatial_{spatial_res}-temporal_{time_res}'\n",
    "        output_filenames.append(output_filename_base)\n",
    "\n",
    "        notebook = PROCESSING_NOTEBOOK_FILE  # Replace with your notebook filename\n",
    "        params = {\n",
    "            \"TIME_RESOLUTION\": time_res,\n",
    "            \"SPATIAL_RESOLUTION\": spatial_res,\n",
    "            \"OUTPUT_FILENAME_BASE\": output_filename_base\n",
    "        }\n",
    "        notebooks_and_params.append((notebook, params))\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(execute_notebook, nb, params) for nb, params in notebooks_and_params]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "# Print exception details\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "datasets = {}\n",
    "\n",
    "for filepath in output_filenames:\n",
    "    filename = f'{os.path.basename(filepath)}_census'\n",
    "    datasets[filename] = preprocess_data(pd.read_csv(f'{filepath}_census.csv'))\n",
    "\n",
    "    filename = f'{os.path.basename(filepath)}_h3'\n",
    "    datasets[filename] = preprocess_data(pd.read_csv(f'{filepath}_h3.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Hyperparameter Tuning\n",
    "This function enables us to do hyperparameter tuning for any model in the sklearn universe. We have the choice to either do a RandomizedGridSearch with cross validation or a standard GridSearch, the latter is computationally heavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(param_grid, model, X, y, randomized=False):\n",
    "    if randomized:\n",
    "        grid = RandomizedSearchCV(model, param_grid)\n",
    "    else:\n",
    "        grid = GridSearchCV(model, param_grid, verbose=3)\n",
    "\n",
    "    grid.fit(X, y)\n",
    "    print(f\"Best params: {grid.best_params_}\")\n",
    "    print(f\"Scoring: {grid.best_score_}\")\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation function:\n",
    "def evaluate_model(y, y_pred):\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do SVR to estimate demand given our features. The issue is that the implementation of our SVR is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets, we can either downsample or use sklearn.svm.LinearSVR which has a more performant implementation. Libsvm scales either O(n_features * n_samples^2) or O(n_features * n_samples^3). We will therefore focus our eperimentations on the linear kernel and only briefly try the other ones with reduced dataset sizes.\n",
    "\n",
    "Overview:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "C=100\n",
    "EPSILON=0.1\n",
    "POLY_DEGREE=3\n",
    "\n",
    "# Training Parameters\n",
    "CACHE_SIZE = 2048 # in MB\n",
    "\n",
    "# Parallel Training\n",
    "MAX_WORKER_THREADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Parallel execution using concurrent.futures\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mMAX_WORKER_THREADS) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m---> 22\u001b[0m     futures \u001b[39m=\u001b[39m [executor\u001b[39m.\u001b[39msubmit(train_linear_svr_models, dataset\u001b[39m.\u001b[39mkey(), dataset\u001b[39m.\u001b[39mvalue(), C, EPSILON, CACHE_SIZE) \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets]\n\u001b[0;32m     24\u001b[0m \u001b[39m# Wait for all futures to complete\u001b[39;00m\n\u001b[0;32m     25\u001b[0m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mwait(futures)\n",
      "Cell \u001b[1;32mIn[21], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Parallel execution using concurrent.futures\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mMAX_WORKER_THREADS) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m---> 22\u001b[0m     futures \u001b[39m=\u001b[39m [executor\u001b[39m.\u001b[39msubmit(train_linear_svr_models, dataset\u001b[39m.\u001b[39;49mkey(), dataset\u001b[39m.\u001b[39mvalue(), C, EPSILON, CACHE_SIZE) \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets]\n\u001b[0;32m     24\u001b[0m \u001b[39m# Wait for all futures to complete\u001b[39;00m\n\u001b[0;32m     25\u001b[0m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mwait(futures)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'key'"
     ]
    }
   ],
   "source": [
    "def train_linear_svr_models(dataset_name, dataset, C, EPSILON, CACHE_SIZE):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "\n",
    "    svr_lin = LinearSVR(C=C, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "    \n",
    "    print(f'Fitting for {dataset_name}..')\n",
    "    svr_lin.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Predicting for {dataset_name}..')\n",
    "    y_lin = svr_rbf.predict(X_test)\n",
    "\n",
    "    print(f'Evaluating for {dataset_name}..')\n",
    "    metrics[dataset_name] = evaluate_model(y_test, y_lin)\n",
    "    models[dataset_name] = svr_lin\n",
    "\n",
    "\n",
    "metrics = {}\n",
    "models = {}\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(train_linear_svr_models, dataset.key(), dataset.value(), C, EPSILON, CACHE_SIZE) for dataset in datasets]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "# Print exception details\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVR models with different kernels\n",
    "# Important - Only \n",
    "svr_rbf = SVR(kernel='rbf', C=C, gamma=C, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "#svr_lin = LinearSVR(C=C, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "#svr_poly = SVR(kernel='poly', C=C, degree=POLY_DEGREE, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "\n",
    "# Fit the models\n",
    "print('Fitting')\n",
    "#svr_lin.fit(X_val, y_val)\n",
    "#svr_poly.fit(X, y)\n",
    "\n",
    "y_rbf = svr_rbf.predict(X_test)\n",
    "print('Predicting')\n",
    "#y_lin = svr_lin.predict(X_test)\n",
    "#y_poly = svr_poly.predict(X_test)\n",
    "print()\n",
    "\n",
    "# Problem: for normal SVMs, the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
