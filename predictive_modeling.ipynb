{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import h3\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import papermill as pm\n",
    "import concurrent.futures\n",
    "import random\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, train_ratio=0.75, validation_ratio=0.15, random_state=42):\n",
    "    features = ['demand_h-2', 'demand_h-24', 'hour_sin', 'hour_cos', 'weekend', 'season_sin', 'season_cos', 'public_holiday', 'temperature', 'precip']\n",
    "    features_to_scale = ['demand_h-2', 'demand_h-24', 'temperature', 'precip']\n",
    "    target = 'demand'\n",
    "    \n",
    "    # Copy the input DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Select features and target\n",
    "    X = df_copy[features]\n",
    "    y = df_copy[target]\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=(1 - train_ratio), random_state=random_state)\n",
    "    X_val_unscaled, X_test_unscaled, y_val, y_test = train_test_split(X_test_unscaled, y_test, test_size=validation_ratio / (validation_ratio + test_ratio), random_state=random_state)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_unscaled[features_to_scale])\n",
    "\n",
    "    X_train = X_train_unscaled.copy()\n",
    "    X_val = X_val_unscaled.copy()\n",
    "    X_test = X_test_unscaled.copy()\n",
    "\n",
    "    X_train[features_to_scale] = scaler.transform(X_train_unscaled[features_to_scale])\n",
    "    X_val[features_to_scale] = scaler.transform(X_val_unscaled[features_to_scale])\n",
    "    X_test[features_to_scale] = scaler.transform(X_test_unscaled[features_to_scale])\n",
    "\n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Datasets from Feature Engineering\n",
    "# This takes around ~ 25-35 Minutes and will fill you RAM and CPU nearly completely. \n",
    "\n",
    "# Constants\n",
    "TIME_RESOLUTIONS = ['1H', '2H', '6H', '24H']\n",
    "SPATIAL_RESOLUTIONS = [6, 7, 8]\n",
    "DATASET_SUFFIX = ['_h3', '_census']\n",
    "PROCESSING_NOTEBOOK_FILE = './predicitve_feature_engineering.ipynb'\n",
    "FILE_BASE_NAME='./data/predictive/dataset'\n",
    "# Max 4 on 32 GB Ram (Adrians Machine)\n",
    "MAX_WORKER_THREADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ^ supress notebook outputs as to not get spammed by 12 Data preparation notebooks. Output can be found under /data/notebook_outs\n",
    "\n",
    "output_filenames = []\n",
    "\n",
    "# Function to execute a notebook with given parameters\n",
    "def execute_notebook(notebook, params):\n",
    "    output_notebook = f\"./data/notebook_outs/output_{random.randint(1, 100)}\"\n",
    "    pm.execute_notebook(notebook, output_notebook, parameters=params)\n",
    "\n",
    "# Generate notebooks and parameters\n",
    "notebooks_and_params = []\n",
    "for time_res in TIME_RESOLUTIONS:\n",
    "    for spatial_res in SPATIAL_RESOLUTIONS:\n",
    "        output_filename_base = f'{FILE_BASE_NAME}-spatial_{spatial_res}-temporal_{time_res}'\n",
    "        output_filenames.append(output_filename_base)\n",
    "\n",
    "        notebook = PROCESSING_NOTEBOOK_FILE  # Replace with your notebook filename\n",
    "        params = {\n",
    "            \"TIME_RESOLUTION\": time_res,\n",
    "            \"SPATIAL_RESOLUTION\": spatial_res,\n",
    "            \"OUTPUT_FILENAME_BASE\": output_filename_base\n",
    "        }\n",
    "        notebooks_and_params.append((notebook, params))\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(execute_notebook, nb, params) for nb, params in notebooks_and_params]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "# Print exception details\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/predictive/dataset-spatial_6-temporal_1H.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m filepath \u001b[39min\u001b[39;00m output_filenames:\n\u001b[0;32m      5\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(filepath)\u001b[39m}\u001b[39;00m\u001b[39m_census\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     datasets[filename] \u001b[39m=\u001b[39m preprocess_data(pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilepath\u001b[39m}\u001b[39;49;00m\u001b[39m.csv\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(filepath)\u001b[39m}\u001b[39;00m\u001b[39m_h3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      9\u001b[0m     datasets[filename] \u001b[39m=\u001b[39m preprocess_data(pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\aaa\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\aaa\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\aaa\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\aaa\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Adrian\\anaconda3\\envs\\aaa\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/predictive/dataset-spatial_6-temporal_1H.csv'"
     ]
    }
   ],
   "source": [
    "# Import datasets\n",
    "datasets = {}\n",
    "\n",
    "for filepath in output_filenames:\n",
    "    filename = f'{os.path.basename(filepath)}_census'\n",
    "    datasets[filename] = preprocess_data(pd.read_csv(f'{filepath}.csv'))\n",
    "\n",
    "    filename = f'{os.path.basename(filepath)}_h3'\n",
    "    datasets[filename] = preprocess_data(pd.read_csv(f'{filepath}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Hyperparameter Tuning\n",
    "This function enables us to do hyperparameter tuning for any model in the sklearn universe. We have the choice to either do a RandomizedGridSearch with cross validation or a standard GridSearch, the latter is computationally heavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(param_grid, model, X, y, randomized=False):\n",
    "    if randomized:\n",
    "        grid = RandomizedSearchCV(model, param_grid)\n",
    "    else:\n",
    "        grid = GridSearchCV(model, param_grid, verbose=3)\n",
    "\n",
    "    grid.fit(X, y)\n",
    "    print(f\"Best params: {grid.best_params_}\")\n",
    "    print(f\"Scoring: {grid.best_score_}\")\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation function:\n",
    "def evaluate_model(y, y_pred):\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do SVR to estimate demand given our features. The issue is that the implementation of our SVR is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets, we can either downsample or use sklearn.svm.LinearSVR which has a more performant implementation. Libsvm scales either O(n_features * n_samples^2) or O(n_features * n_samples^3). We will therefore focus our eperimentations on the linear kernel and only briefly try the other ones with reduced dataset sizes.\n",
    "\n",
    "Overview:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "C=100\n",
    "EPSILON=0.1\n",
    "POLY_DEGREE=3\n",
    "\n",
    "# Training Parameters\n",
    "CACHE_SIZE = 2048 # in MB\n",
    "\n",
    "# Parallel Training\n",
    "MAX_WORKER_THREADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Parallel execution using concurrent.futures\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mMAX_WORKER_THREADS) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m---> 22\u001b[0m     futures \u001b[39m=\u001b[39m [executor\u001b[39m.\u001b[39msubmit(train_linear_svr_models, dataset\u001b[39m.\u001b[39mkey(), dataset\u001b[39m.\u001b[39mvalue(), C, EPSILON, CACHE_SIZE) \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets]\n\u001b[0;32m     24\u001b[0m \u001b[39m# Wait for all futures to complete\u001b[39;00m\n\u001b[0;32m     25\u001b[0m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mwait(futures)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "def train_linear_svr_models(dataset_name, dataset, C, EPSILON, CACHE_SIZE):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "\n",
    "    svr_lin = LinearSVR(C=C, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "    \n",
    "    print(f'Fitting for {dataset_name}..')\n",
    "    svr_lin.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Predicting for {dataset_name}..')\n",
    "    y_lin = svr_rbf.predict(X_test)\n",
    "\n",
    "    print(f'Evaluating for {dataset_name}..')\n",
    "    metrics[dataset_name] = evaluate_model(y_test, y_lin)\n",
    "    models[dataset_name] = svr_lin\n",
    "\n",
    "\n",
    "metrics = {}\n",
    "models = {}\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(train_linear_svr_models, dataset.key(), dataset.value(), C, EPSILON, CACHE_SIZE) for dataset in datasets]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "# Print exception details\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVR models with different kernels\n",
    "# Important - Only \n",
    "svr_rbf = SVR(kernel='rbf', C=C, gamma=C, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "#svr_lin = LinearSVR(C=C, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "#svr_poly = SVR(kernel='poly', C=C, degree=POLY_DEGREE, epsilon=EPSILON, cache_size=CACHE_SIZE)\n",
    "\n",
    "# Fit the models\n",
    "print('Fitting')\n",
    "#svr_lin.fit(X_val, y_val)\n",
    "#svr_poly.fit(X, y)\n",
    "\n",
    "y_rbf = svr_rbf.predict(X_test)\n",
    "print('Predicting')\n",
    "#y_lin = svr_lin.predict(X_test)\n",
    "#y_poly = svr_poly.predict(X_test)\n",
    "print()\n",
    "\n",
    "# Problem: for normal SVMs, the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
