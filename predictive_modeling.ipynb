{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import h3\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import papermill as pm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import concurrent.futures\n",
    "import random\n",
    "import os \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import copy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RANDOM_STATE=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, train_ratio=0.75, validation_ratio=0.15):\n",
    "    features = ['demand_h-2', 'demand_h-24', 'hour_sin', 'hour_cos', 'weekend', 'season_sin', 'season_cos', 'public_holiday', 'temperature', 'precip']\n",
    "    features_to_scale = ['demand_h-2', 'demand_h-24', 'temperature', 'precip']\n",
    "    target = 'demand'\n",
    "    \n",
    "    # Copy the input DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Select features and target\n",
    "    X = df_copy[features]\n",
    "    y = df_copy[target]\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    test_ratio = (1-train_ratio)-validation_ratio\n",
    "    X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=(1 - train_ratio), random_state=RANDOM_STATE)\n",
    "    X_val_unscaled, X_test_unscaled, y_val, y_test = train_test_split(X_test_unscaled, y_test, test_size=test_ratio / (validation_ratio + test_ratio), random_state=RANDOM_STATE)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_unscaled[features_to_scale])\n",
    "\n",
    "    X_train = X_train_unscaled.copy()\n",
    "    X_val = X_val_unscaled.copy()\n",
    "    X_test = X_test_unscaled.copy()\n",
    "\n",
    "    X_train[features_to_scale] = scaler.transform(X_train_unscaled[features_to_scale])\n",
    "    X_val[features_to_scale] = scaler.transform(X_val_unscaled[features_to_scale])\n",
    "    X_test[features_to_scale] = scaler.transform(X_test_unscaled[features_to_scale])\n",
    "\n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Datasets from Feature Engineering\n",
    "# This takes around ~ 25-35 Minutes and will fill you RAM and CPU nearly completely. \n",
    "\n",
    "# Constants\n",
    "TIME_RESOLUTIONS = ['1H', '2H', '6H', '24H']\n",
    "SPATIAL_RESOLUTIONS = [6, 7, 8]\n",
    "DATASET_SUFFIX = ['_h3', '_census']\n",
    "PROCESSING_NOTEBOOK_FILE = './predicitve_feature_engineering.ipynb'\n",
    "FILE_BASE_NAME='./data/predictive/dataset'\n",
    "RUN_FEATURE_ENGINERRING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ^ supress notebook outputs as to not get spammed by 12 Data preparation notebooks. Output can be found under /data/notebook_outs\n",
    "\n",
    "output_filenames = []\n",
    "\n",
    "# Function to execute a notebook with given parameters\n",
    "def execute_notebook(notebook, params):\n",
    "    output_notebook = f\"./data/notebook_outs/output_{random.randint(1, 100)}\"\n",
    "    pm.execute_notebook(notebook, output_notebook, parameters=params)\n",
    "\n",
    "# Generate notebooks and parameters\n",
    "notebooks_and_params = []\n",
    "for time_res in TIME_RESOLUTIONS:\n",
    "    for spatial_res in SPATIAL_RESOLUTIONS:\n",
    "        output_filename_base = f'{FILE_BASE_NAME}-spatial_{spatial_res}-temporal_{time_res}'\n",
    "        output_filenames.append(output_filename_base)\n",
    "\n",
    "        notebook = PROCESSING_NOTEBOOK_FILE  # Replace with your notebook filename\n",
    "        params = {\n",
    "            \"TIME_RESOLUTION\": time_res,\n",
    "            \"SPATIAL_RESOLUTION\": spatial_res,\n",
    "            \"OUTPUT_FILENAME_BASE\": output_filename_base\n",
    "        }\n",
    "        notebooks_and_params.append((notebook, params))\n",
    "\n",
    "if (RUN_FEATURE_ENGINERRING):\n",
    "    \n",
    "    # Max 4 on 32 GB Ram (Adrians Machine)\n",
    "    MAX_WORKER_THREADS = 4\n",
    "    # Parallel execution using concurrent.futures\n",
    "    with concurrent.futures.ThreadPoolExecutor(MAX_WORKER_THREADS) as executor:\n",
    "        futures = [executor.submit(execute_notebook, nb, params) for nb, params in notebooks_and_params]\n",
    "\n",
    "    # Wait for all futures to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "    # Print exception details\n",
    "    for future in futures:\n",
    "        exception = future.exception()\n",
    "        if exception:\n",
    "            print(f\"Exception in future: {exception}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import datasets\n",
    "datasets = {}\n",
    "\n",
    "for filepath in output_filenames:\n",
    "    filename = f'{os.path.basename(filepath)}_census'\n",
    "    datasets[filename] = preprocess_data(pd.read_csv(f'{filepath}_census.csv'))\n",
    "\n",
    "    filename = f'{os.path.basename(filepath)}_h3'\n",
    "    datasets[filename] = preprocess_data(pd.read_csv(f'{filepath}_h3.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Hyperparameter Tuning\n",
    "This function enables us to do hyperparameter tuning for any model in the sklearn universe. We have the choice to either do a RandomizedGridSearch with cross validation or a standard GridSearch, the latter is computationally heavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(param_grid, model, X, y, randomized=False):\n",
    "    if randomized:\n",
    "        grid = RandomizedSearchCV(model, param_grid)\n",
    "    else:\n",
    "        grid = GridSearchCV(model, param_grid, verbose=3)\n",
    "\n",
    "    grid.fit(X, y)\n",
    "    print(f\"Best params: {grid.best_params_}\")\n",
    "    print(f\"Scoring: {grid.best_score_}\")\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation function:\n",
    "def evaluate_model(y, y_pred):\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do SVR to estimate demand given our features. The issue is that the implementation of our SVR is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets, we can either downsample or use sklearn.svm.LinearSVR which has a more performant implementation. Libsvm scales either O(n_features * n_samples^2) or O(n_features * n_samples^3). We will therefore focus our eperimentations on the linear kernel and only briefly try the other ones with reduced dataset sizes.\n",
    "\n",
    "Overview:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - General\n",
    "C=1\n",
    "EPSILON=0.1\n",
    "\n",
    "# Hyperparameters - Linear\n",
    "MAX_ITER = 2500 # default 1000\n",
    "\n",
    "POLY_DEGREE=3\n",
    "\n",
    "# Training Parameters\n",
    "CACHE_SIZE = 2048 # in MB\n",
    "\n",
    "# Parallel Training \n",
    "# rougly 4 Min per Model, can run ~ 8 in parallel -> 30 Min for LinSVR Training\n",
    "MAX_WORKER_THREADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_svr_models(dataset_name, dataset, C, EPSILON):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "\n",
    "    # Dual = automatically select the fastest problem (dual or primary problem) based on #features vs #values\n",
    "    svr_lin = LinearSVR(C=C, epsilon=EPSILON, dual='auto', max_iter=MAX_ITER)\n",
    "    \n",
    "    print(f'Fitting for {dataset_name}..')\n",
    "    svr_lin.fit(X_train, y_train)\n",
    "\n",
    "    print(f'Predicting for {dataset_name}..')\n",
    "    y_lin = svr_lin.predict(X_test)\n",
    "\n",
    "    print(f'Evaluating for {dataset_name}..')\n",
    "    svr_metrics[dataset_name] = evaluate_model(y_test, y_lin)\n",
    "    svr_models[dataset_name] = svr_lin\n",
    "\n",
    "\n",
    "def train_with_dataset_and_parameters(dataset_key, dataset_values):\n",
    "    train_linear_svr_models(dataset_key, dataset_values, C, EPSILON)\n",
    "\n",
    "svr_metrics = {}\n",
    "svr_models = {}\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(train_with_dataset_and_parameters, dataset_name, dataset_values)\n",
    "               for dataset_name, dataset_values in datasets.items()]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear SVR\n",
    "As mentioned previously, we will just briefly venture into the realm of alternative kernels, as the library does not offer an efficient implmentation for rbf, sigmoid or poly kernels. With a non-linear runtime, we have to massively cut down on our dataset to get a reasonable training time. The documentation of the scikit-learn implementation of the SVR already suggests the use-cases of more complex kernel functions: If you have a limited dataset \"at most a couple of 10000\" (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) and want to maximize the performance, using an more complex kernel function like rbf can be beneficial. However, if a large amount of data is available, in the case of the dataset present, it is more effective to train with the full dataset on the linear kernel, instead of using a more intricate kernel with a smaller dataset.\n",
    "\n",
    "To generate a reduced trainingset, we will run the train_test_split function again and discard leftover data. We will run half of the datasets with the poly and the other half with the rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - General\n",
    "C=1\n",
    "EPSILON=0.1\n",
    "\n",
    "# Hyperparameters - Linear\n",
    "MAX_ITER = 2500 # default 1000\n",
    "\n",
    "POLY_DEGREE=3\n",
    "\n",
    "# Training Parameters\n",
    "CACHE_SIZE = 2048 # in MB\n",
    "MAX_X_TRAIN_REDUCED_SIZE = 50000\n",
    "# Parallel Training \n",
    "# rougly 4 Min per Model, can run ~ 8 in parallel -> 30 Min for LinSVR Training\n",
    "MAX_WORKER_THREADS = 8\n",
    "\n",
    "# split datasets\n",
    "def random_split_dict(dictionary, split_ratio=0.5):\n",
    "    copied_dict = copy.deepcopy(dictionary)\n",
    "    \n",
    "    keys = list(copied_dict.keys())\n",
    "    random.shuffle(keys)\n",
    "    \n",
    "    split_index = int(len(keys) * split_ratio)\n",
    "    \n",
    "    dict_part1 = {key: copied_dict[key] for key in keys[:split_index]}\n",
    "    dict_part2 = {key: copied_dict[key] for key in keys[split_index:]}\n",
    "    \n",
    "    return dict_part1, dict_part2\n",
    "\n",
    "datasets_reduced_rbf, datasets_reduced_poly = random_split_dict(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF\n",
    "def train_rbf_svr_models(dataset_name, dataset, C, EPSILON):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "\n",
    "    # Calculate the test_size dynamically based on X_train size\n",
    "    if len(X_train) <= MAX_X_TRAIN_REDUCED_SIZE:\n",
    "        test_size = 0.0  # No test set if X_train is smaller than or equal to MAX_X_TRAIN_REDUCED_SIZE\n",
    "    else:\n",
    "        test_size = 1.0 - (MAX_X_TRAIN_REDUCED_SIZE / len(X_train))\n",
    "\n",
    "    # reduce dataset size\n",
    "    X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(X_train, y_train, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Dual = automatically select the fastest problem (dual or primary problem) based on #features vs #values\n",
    "    svr_rbf = SVR(kernel='rbf', C=C, epsilon=EPSILON, max_iter=MAX_ITER, cache_size=2048)\n",
    "    \n",
    "    print(f'Fitting for {dataset_name}..')\n",
    "    svr_rbf.fit(X_train_reduced, y_train_reduced)\n",
    "\n",
    "    print(f'Predicting for {dataset_name}..')\n",
    "    y_preds = svr_rbf.predict(X_test_reduced)\n",
    "\n",
    "    print(f'Evaluating for {dataset_name}..')\n",
    "    svr_metrics[dataset_name] = evaluate_model(y_test_reduced, y_preds)\n",
    "    svr_models[dataset_name] = svr_rbf\n",
    "\n",
    "\n",
    "def train_with_dataset_and_parameters(dataset_key, dataset_values):\n",
    "    train_rbf_svr_models(dataset_key, dataset_values, C, EPSILON)\n",
    "\n",
    "rbf_metrics = {}\n",
    "rbf_models = {}\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(train_with_dataset_and_parameters, dataset_name, dataset_values)\n",
    "               for dataset_name, dataset_values in datasets_reduced_rbf.items()]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Poly\n",
    "def train_poly_svr_models(dataset_name, dataset, C, EPSILON):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "\n",
    "     # Calculate the test_size dynamically based on X_train size\n",
    "    if len(X_train) <= MAX_X_TRAIN_REDUCED_SIZE:\n",
    "        test_size = 0.001  # No test set if X_train is smaller than or equal to MAX_X_TRAIN_REDUCED_SIZE\n",
    "    else:\n",
    "        test_size = 1.0 - (MAX_X_TRAIN_REDUCED_SIZE / len(X_train))\n",
    "\n",
    "    # reduce dataset size\n",
    "    X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(X_train, y_train, test_size=test_size, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Dual = automatically select the fastest problem (dual or primary problem) based on #features vs #values\n",
    "    svr_poly = SVR(kernel='poly', C=C, epsilon=EPSILON, max_iter=MAX_ITER, cache_size=2048)\n",
    " \n",
    "    print(f'Fitting for {dataset_name}..')\n",
    "    svr_poly.fit(X_train_reduced, y_train_reduced)\n",
    "\n",
    "    print(f'Predicting for {dataset_name}..')\n",
    "    y_preds = svr_poly.predict(X_test_reduced)\n",
    "\n",
    "    print(f'Evaluating for {dataset_name}..')\n",
    "    svr_metrics[dataset_name] = evaluate_model(y_test_reduced, y_preds)\n",
    "    svr_models[dataset_name] = svr_poly\n",
    "\n",
    "\n",
    "def train_with_dataset_and_parameters(dataset_key, dataset_values):\n",
    "    train_poly_svr_models(dataset_key, dataset_values, C, EPSILON)\n",
    "\n",
    "poly_metrics = {}\n",
    "poly_models = {}\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(train_with_dataset_and_parameters, dataset_name, dataset_values)\n",
    "               for dataset_name, dataset_values in datasets_reduced_poly.items()]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE=0.0025 # higher learn rate as we have a bad gpu\n",
    "OPTIMIZER=keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS='mean_squared_error'\n",
    "\n",
    "\n",
    "# Training Multiprocessing\n",
    "# It seams on a 16 core cpu we can increase this to more than 8\n",
    "MAX_WORKER_THREADS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "# as they are all the same and have no order in the dict we will just grab any element and get the shape of the train dataframe\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = datasets[random.choice(list(datasets.keys()))]\n",
    "\n",
    "model_abstract = keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),  # Input layer for time series data\n",
    "    layers.Dense(128, activation='relu'),     # Hidden layer 1\n",
    "    layers.Dropout(0.3),                      # Dropout layer for regularization\n",
    "    layers.Dense(64, activation='relu'),      # Hidden layer 2\n",
    "    layers.Dropout(0.2),                      # Dropout layer for regularization\n",
    "    layers.Dense(32, activation='relu'),      # Hidden layer 3\n",
    "    layers.Dropout(0.1),                      # Dropout layer for regularization\n",
    "    layers.Dense(1)                           # Output layer\n",
    "])\n",
    "model_abstract.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "def train_with_dataset_and_parameters(dataset_name, dataset_values):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset_values\n",
    "    \n",
    "    model = keras.models.clone_model(model_abstract)\n",
    "\n",
    "    OPTIMIZER.build(model.trainable_variables)\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "    nn_metrics[dataset_name] = evaluate_model(y_test, y_preds)\n",
    "    nn_models[dataset_name] = model\n",
    "    nn_importance[dataset_name] = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "\n",
    "nn_metrics = {}\n",
    "nn_models = {}\n",
    "nn_importance = {}\n",
    "\n",
    "# Parallel execution using concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKER_THREADS) as executor:\n",
    "    futures = [executor.submit(train_with_dataset_and_parameters, dataset_name, dataset_values)\n",
    "               for dataset_name, dataset_values in datasets_reduced_poly.items()]\n",
    "\n",
    "# Wait for all futures to complete\n",
    "concurrent.futures.wait(futures)\n",
    "\n",
    "for future in futures:\n",
    "    exception = future.exception()\n",
    "    if exception:\n",
    "        print(f\"Exception in future: {exception}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
